{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0dd6d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import OpenAI, VectorDBQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import os\n",
    "import streamlit as st\n",
    "import logging\n",
    "\n",
    "# import config\n",
    "# import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d42ec34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "OPENAI_API_KEY = \"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\"\n",
    "PERSIST_DIR = '../raw_data/vector'\n",
    "LOGS_FILE = '../raw_data/logs.logs'\n",
    "prompt_template = \"\"\"You are a personal medical Bot assistant for answering any questions about documents.\n",
    "Use medical technical language.\n",
    "You are given a question and a set of documents.\n",
    "If the user's question requires you to provide specific information from the documents, give your answer based only on the examples provided below. DON'T generate an answer that is NOT written in the provided examples.\n",
    "If you don't find the answer to the user's question with the examples provided to you below, answer that you didn't find the answer in the documentation and propose him to rephrase his query with more details.\n",
    "Use bullet points if you have to make a list, only if necessary.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "DOCUMENTS:\n",
    "=========\n",
    "{context}\n",
    "=========\n",
    "Finish by proposing your help for anything else.\n",
    "\"\"\"\n",
    "k = 4  # number of chunks to consider when generating answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f63048c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging with the specified configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOGS_FILE),\n",
    "        logging.StreamHandler(),\n",
    "    ],\n",
    ")\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f70a14d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "13581ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1a91c011",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(path = '../raw_data', glob='*.pdf')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a4dce27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 14:20:38,212 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "#split the text\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "#split the documents into chunks of size 1000 using the splitter\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "#create a vector store from the chunks using the OpenAI embeddings and a chroma object\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key = OPENAI_API_KEY)\n",
    "docsearch = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5b8e2395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(prompt: str, presist_directory: str = PERSIST_DIR) -> str:\n",
    "    API = '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'\n",
    "    LOGGER.info(f\"Start answerin based on prompt: {prompt}\")\n",
    "    \n",
    "    #promt template\n",
    "    \n",
    "    prompt_template_2 = PromptTemplate(template = prompt_template, input_variables = ['context','question'])\n",
    "    \n",
    "    #load qa chain\n",
    "    doc_chain = load_qa_chain(\n",
    "        llm = OpenAI(\n",
    "            openai_api_key= API,\n",
    "            model_name = \"text-davinci-003\",\n",
    "            temperature = 0.7,\n",
    "            max_tokens = 300\n",
    "        ),\n",
    "        prompt=prompt_template_2\n",
    "    \n",
    "    )\n",
    "    #log a message indicating the number of chunks to be considered\n",
    "    LOGGER.info(f\"The top {k} chunks are considered to answer the user's query\")\n",
    "    \n",
    "    #create the vectorDBQA object\n",
    "    qa = VectorDBQA (vectorstore = docsearch, combine_documents_chain = doc_chain, k = k)\n",
    "    \n",
    "    #call the vectorDBQA object to generate an answer to the prompt\n",
    "    result = qa({\"query\": prompt})\n",
    "    answer = result['result']\n",
    "    \n",
    "    #log a message indicating the answer \n",
    "    LOGGER.info(f\"The returned answer is {answer}\")\n",
    "    \n",
    "    #log a message indicating the function has finished and returned an answer\n",
    "    LOGGER.info(f\"Answering module over.\")\n",
    "    \n",
    "    return answer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "29abd99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 14:35:30,571 - INFO - Start answerin based on prompt: what is the content on the sample.pdf file\n",
      "2023-07-23 14:35:30,577 - INFO - The top 4 chunks are considered to answer the user's query\n",
      "2023-07-23 14:35:33,711 - INFO - The returned answer is \n",
      "Answer: The content of the sample.pdf file is a medical report with the patient's particulars, doctor's particulars, doctor-patient relationship, and a declaration.\n",
      "2023-07-23 14:35:33,714 - INFO - Answering module over.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nAnswer: The content of the sample.pdf file is a medical report with the patient's particulars, doctor's particulars, doctor-patient relationship, and a declaration.\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer('what is the content on the sample.pdf file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "20a5e59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 14:43:00,047 - INFO - Start answerin based on prompt: give me a clinical report on the patient Mr Tan Ah Kow\n",
      "2023-07-23 14:43:00,053 - INFO - The top 4 chunks are considered to answer the user's query\n",
      "2023-07-23 14:43:03,332 - INFO - The returned answer is \n",
      "Answer: Based on the provided documents, Mr Tan Ah Kow is a 55 year old man, who is divorced, and unemployed. He has had hypertension and hyperlipidemia since 1990 and suffered several strokes in 2005. He subsequently developed heart problems (cardiomyopathy), cardiac failure and chronic renal disease and was treated in ABC Hospital. The doctor in charge of Mr Tan Ah Kow is Tan Ah Moi since November 2010 and they have seen each other regularly since then, on average once or twice a year.\n",
      "\n",
      "I hope this answers your question. Is there anything else I can help you with?\n",
      "2023-07-23 14:43:03,334 - INFO - Answering module over.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Based on the provided documents, Mr Tan Ah Kow is a 55 year old man, who is divorced, and unemployed. He has had hypertension and hyperlipidemia since 1990 and suffered several strokes in 2005. He subsequently developed heart problems (cardiomyopathy), cardiac failure and chronic renal disease and was treated in ABC Hospital. The doctor in charge of Mr Tan Ah Kow is Tan Ah Moi since November 2010 and they have seen each other regularly since then, on average once or twice a year.\n",
      "\n",
      "I hope this answers your question. Is there anything else I can help you with?\n"
     ]
    }
   ],
   "source": [
    "print(answer('give me a clinical report on the patient Mr Tan Ah Kow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea09e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
