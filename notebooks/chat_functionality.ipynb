{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a8c4e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import OpenAI, VectorDBQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import os\n",
    "import streamlit as st\n",
    "import logging\n",
    "import config\n",
    "# import config\n",
    "# import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "08dfe03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "OPENAI_API_KEY = \"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\"\n",
    "PERSIST_DIR = '../raw_data/vector'\n",
    "LOGS_FILE = '../raw_data/logs.logs'\n",
    "prompt_template = \"\"\"You are a personal medical Bot assistant for answering any questions about documents.\n",
    "Use medical technical language.\n",
    "You are given a question and a set of documents.\n",
    "If the user's question requires you to provide specific information from the documents, give your answer based only on the examples provided below. DON'T generate an answer that is NOT written in the provided examples.\n",
    "If you don't find the answer to the user's question with the examples provided to you below, answer that you didn't find the answer in the documentation and propose him to rephrase his query with more details.\n",
    "Use bullet points if you have to make a list, only if necessary.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "DOCUMENTS:\n",
    "=========\n",
    "{context}\n",
    "=========\n",
    "Finish by proposing your help for anything else.\n",
    "\"\"\"\n",
    "k = 4  # number of chunks to consider when generating answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "11c4fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging with the specified configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOGS_FILE),\n",
    "        logging.StreamHandler(),\n",
    "    ],\n",
    ")\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aabbaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d585fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "49243f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(path = '../raw_data', glob='*.pdf')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cee1ac0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 14:20:38,212 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "#split the text\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "#split the documents into chunks of size 1000 using the splitter\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "#create a vector store from the chunks using the OpenAI embeddings and a chroma object\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key = OPENAI_API_KEY)\n",
    "docsearch = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "54fa3d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(prompt: str, presist_directory: str = PERSIST_DIR) -> str:\n",
    "    API = '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'\n",
    "    LOGGER.info(f\"Start answerin based on prompt: {prompt}\")\n",
    "    \n",
    "    #promt template\n",
    "    \n",
    "    prompt_template_2 = PromptTemplate(template = prompt_template, input_variables = ['context','question'])\n",
    "    \n",
    "    #load qa chain\n",
    "    doc_chain = load_qa_chain(\n",
    "        llm = OpenAI(\n",
    "            openai_api_key= os.environ.get('OPENAI_API'),\n",
    "            model_name = \"text-davinci-003\",\n",
    "            temperature = 0.7,\n",
    "            max_tokens = 300\n",
    "        ),\n",
    "        prompt=prompt_template_2\n",
    "    \n",
    "    )\n",
    "    #log a message indicating the number of chunks to be considered\n",
    "    LOGGER.info(f\"The top {k} chunks are considered to answer the user's query\")\n",
    "    \n",
    "    #create the vectorDBQA object\n",
    "    qa = VectorDBQA (vectorstore = docsearch, combine_documents_chain = doc_chain, k = k)\n",
    "    \n",
    "    #call the vectorDBQA object to generate an answer to the prompt\n",
    "    result = qa({\"query\": prompt})\n",
    "    answer = result['result']\n",
    "    \n",
    "    #log a message indicating the answer \n",
    "    LOGGER.info(f\"The returned answer is {answer}\")\n",
    "    \n",
    "    #log a message indicating the function has finished and returned an answer\n",
    "    LOGGER.info(f\"Answering module over.\")\n",
    "    \n",
    "    return answer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "210a94c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 14:35:30,571 - INFO - Start answerin based on prompt: what is the content on the sample.pdf file\n",
      "2023-07-23 14:35:30,577 - INFO - The top 4 chunks are considered to answer the user's query\n",
      "2023-07-23 14:35:33,711 - INFO - The returned answer is \n",
      "Answer: The content of the sample.pdf file is a medical report with the patient's particulars, doctor's particulars, doctor-patient relationship, and a declaration.\n",
      "2023-07-23 14:35:33,714 - INFO - Answering module over.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nAnswer: The content of the sample.pdf file is a medical report with the patient's particulars, doctor's particulars, doctor-patient relationship, and a declaration.\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer('what is the content on the sample.pdf file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d8d5caf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 15:11:46,856 - INFO - Start answerin based on prompt: give me a clinical report on the patient Mr Tan Ah Kow\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgive me a clinical report on the patient Mr Tan Ah Kow\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[100], line 11\u001b[0m, in \u001b[0;36manswer\u001b[0;34m(prompt, presist_directory)\u001b[0m\n\u001b[1;32m      7\u001b[0m prompt_template_2 \u001b[38;5;241m=\u001b[39m PromptTemplate(template \u001b[38;5;241m=\u001b[39m prompt_template, input_variables \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#load qa chain\u001b[39;00m\n\u001b[1;32m     10\u001b[0m doc_chain \u001b[38;5;241m=\u001b[39m load_qa_chain(\n\u001b[0;32m---> 11\u001b[0m     llm \u001b[38;5;241m=\u001b[39m OpenAI(\n\u001b[1;32m     12\u001b[0m         openai_api_key\u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPENAI_API\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     13\u001b[0m         model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-davinci-003\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m         temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m,\n\u001b[1;32m     15\u001b[0m         max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[1;32m     16\u001b[0m     ),\n\u001b[1;32m     17\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt_template_2\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#log a message indicating the number of chunks to be considered\u001b[39;00m\n\u001b[1;32m     21\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe top \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunks are considered to answer the user\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms query\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/load/serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "print(answer('give me a clinical report on the patient Mr Tan Ah Kow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8bddef86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get('OPENAI_API'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52241988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
