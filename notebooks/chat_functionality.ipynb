{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "29865563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import OpenAI, VectorDBQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import os\n",
    "import streamlit as st\n",
    "import logging\n",
    "\n",
    "# import config\n",
    "# import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e8c3285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b0ecf21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "\n",
    "PERSIST_DIR = '../raw_data/vector'\n",
    "LOGS_FILE = '../raw_data/logs.logs'\n",
    "prompt_template = \"\"\"You are a personal medical Bot assistant for answering any questions about documents.\n",
    "Use medical technical language.\n",
    "You are given a question and a set of documents.\n",
    "If the user's question requires you to provide specific information from the documents, give your answer based only on the examples provided below. DON'T generate an answer that is NOT written in the provided examples.\n",
    "If you don't find the answer to the user's question with the examples provided to you below, answer that you didn't find the answer in the documentation and propose him to rephrase his query with more details.\n",
    "Use bullet points if you have to make a list, only if necessary.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "DOCUMENTS:\n",
    "=========\n",
    "{context}\n",
    "=========\n",
    "Finish by proposing your help for anything else.\n",
    "\"\"\"\n",
    "k = 4  # number of chunks to consider when generating answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8a014257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging with the specified configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOGS_FILE),\n",
    "        logging.StreamHandler(),\n",
    "    ],\n",
    ")\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bb3911d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(path = '../raw_data', glob='*.pdf')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b6e802ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 15:29:19,903 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "#split the text\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "#split the documents into chunks of size 1000 using the splitter\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "#create a vector store from the chunks using the OpenAI embeddings and a chroma object\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key = OPENAI_API_KEY)\n",
    "docsearch = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1306dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(prompt: str, presist_directory: str = PERSIST_DIR) -> str:\n",
    "  \n",
    "    api = os.environ.get('OPENAI_API')\n",
    "    LOGGER.info(f\"Start answerin based on prompt: {prompt}\")\n",
    "    #api = os.environ.get('OPENAI_API')\n",
    "    \n",
    "    #promt template\n",
    "    \n",
    "    prompt_template_2 = PromptTemplate(template = prompt_template, input_variables = ['context','question'])\n",
    "    \n",
    "    #load qa chain\n",
    "    doc_chain = load_qa_chain(\n",
    "        llm = OpenAI(\n",
    "            openai_api_key= api,\n",
    "            model_name = \"text-davinci-003\",\n",
    "            temperature = 0.7,\n",
    "            max_tokens = 300\n",
    "        ),\n",
    "        prompt=prompt_template_2\n",
    "    \n",
    "    )\n",
    "    #log a message indicating the number of chunks to be considered\n",
    "    LOGGER.info(f\"The top {k} chunks are considered to answer the user's query\")\n",
    "    \n",
    "    #create the vectorDBQA object\n",
    "    qa = VectorDBQA (vectorstore = docsearch, combine_documents_chain = doc_chain, k = k)\n",
    "    \n",
    "    #call the vectorDBQA object to generate an answer to the prompt\n",
    "    result = qa({\"query\": prompt})\n",
    "    answer = result['result']\n",
    "    \n",
    "    #log a message indicating the answer \n",
    "    LOGGER.info(f\"The returned answer is {answer}\")\n",
    "    \n",
    "    #log a message indicating the function has finished and returned an answer\n",
    "    LOGGER.info(f\"Answering module over.\")\n",
    "    \n",
    "    return answer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b095c5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 14:35:30,571 - INFO - Start answerin based on prompt: what is the content on the sample.pdf file\n",
      "2023-07-23 14:35:30,577 - INFO - The top 4 chunks are considered to answer the user's query\n",
      "2023-07-23 14:35:33,711 - INFO - The returned answer is \n",
      "Answer: The content of the sample.pdf file is a medical report with the patient's particulars, doctor's particulars, doctor-patient relationship, and a declaration.\n",
      "2023-07-23 14:35:33,714 - INFO - Answering module over.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nAnswer: The content of the sample.pdf file is a medical report with the patient's particulars, doctor's particulars, doctor-patient relationship, and a declaration.\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer('what is the content on the sample.pdf file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "22b9c3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 15:32:06,443 - INFO - Start answerin based on prompt: give me a clinical report on the patient Mr Tan Ah Kow\n",
      "2023-07-23 15:32:06,451 - INFO - The top 4 chunks are considered to answer the user's query\n",
      "2023-07-23 15:32:12,919 - INFO - The returned answer is \n",
      "Based on the provided documents, here is the clinical report for Mr Tan Ah Kow:\n",
      "\n",
      "Source of Information: \n",
      "- Accompanied by his son, Mr Tan Ah Beng, for the examination \n",
      "- Information from Mr Tan Ah Beng \n",
      "\n",
      "Personal Details: \n",
      "- 55 year old man \n",
      "- Divorced \n",
      "- Unemployed \n",
      "- Living with his son, Ah Beng, in Ah Beng’s flat \n",
      "- Used to work as a cleaner \n",
      "\n",
      "Medical History: \n",
      "- Hypertension and hyperlipidemia since 1990 \n",
      "- Several strokes in 2005 \n",
      "- Heart problems (cardiomyopathy) \n",
      "- Cardiac failure \n",
      "- Chronic renal disease \n",
      "- Treated in ABC Hospital \n",
      "\n",
      "I hope this answers your question. If you need any further assistance, please let me know.\n",
      "2023-07-23 15:32:12,923 - INFO - Answering module over.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the provided documents, here is the clinical report for Mr Tan Ah Kow:\n",
      "\n",
      "Source of Information: \n",
      "- Accompanied by his son, Mr Tan Ah Beng, for the examination \n",
      "- Information from Mr Tan Ah Beng \n",
      "\n",
      "Personal Details: \n",
      "- 55 year old man \n",
      "- Divorced \n",
      "- Unemployed \n",
      "- Living with his son, Ah Beng, in Ah Beng’s flat \n",
      "- Used to work as a cleaner \n",
      "\n",
      "Medical History: \n",
      "- Hypertension and hyperlipidemia since 1990 \n",
      "- Several strokes in 2005 \n",
      "- Heart problems (cardiomyopathy) \n",
      "- Cardiac failure \n",
      "- Chronic renal disease \n",
      "- Treated in ABC Hospital \n",
      "\n",
      "I hope this answers your question. If you need any further assistance, please let me know.\n"
     ]
    }
   ],
   "source": [
    "print(answer('give me a clinical report on the patient Mr Tan Ah Kow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d3ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c803edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44889c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
